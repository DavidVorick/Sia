\documentclass[twocolumn]{article}

\begin{document}
\frenchspacing

\title{Sia: Federated Decentralized Storage}

\author{
{\rm David Vorick}\\
Nebulous Labs
}

\maketitle

\subsection*{Abstract}
This paper discusses the desired traits for constructing a federated, decentralized, byzantine fault tolerant file storage system, and then builds up the tools required to implement such a system. Finally, a protocol overview is provided of a system that could possess all of the desired traits.

\section{Introduction}
The traditional model for cloud storage involves one centralized party setting up a data center or a set of data centers and having the consumer trust their data to a single host.
Many times, such in the case of iCloud, the uses of the cloud storage are segmented into a particular ecosystem, or the user is forced to accept terms of service that they either don't read, don't trust, don't like, or don't understand.
New services looking to offer cloud storage need to find a customer base, and are in competition with existing services.
A decentralized approach erases many of these problems.
With decentralized storage, the consumer can use any client that follows the protocol, and is not boxed into unfavorable terms of service or long term contracts.
The client can withdraw the data at any time.
New datacenters can join the network and immediately have a payout for the storage they provide.

We also wish to make our storage system efficient, which will require federating storage.
Maximum efficiency is to have one file be stored on exactly one machine, with no chance of any storage being needlessly redundant throught the entire system.
Uploaders looking for reliability can employ erasure coding before uploading, which protects their file against corruption even in the event that individual components are lost.
The uploader can estimate the required minimum redundancy by looking at the stability statistics across the network.

A decentralized storage platform is likely to run on many datacenters and home connections.
This gives the network great flexibility in providing parallelism to uploaders and downloaders.
A 100MB file can be reasonably broken into several hundred pieces after redundancy is applied, which means downloads can draw from several hundred uploaders simultaneously.
Even where the average connection is slow, the throughput on optimized files will be enormous.

Latency can also be driven down significantly by using high redundancy.
If a small file such as a webpage is stored at a redundancy of 10 or 100, then the closest set of hosts containing the full file is likely to be within just a few milliseconds, regardless of where in the world you are downloading the file from.

By creating a self-regulating price for storage based on supply and demand, competitive and smooth pricing can be maintained.
Traditional services typically have a set price that occasionally drops significantly.
Sia updates the price on a frequent basis based on supply and demand.
Suppliers can look at the price of storage (which directly corresponds with their compensation) and decide whether it is profitable to add storage to the network.
Suppliers who can provide storage for less will jump on the network proftiably and help to drive the price down for consumers.
By looking at the extreme efficiency that Bitcoin has achieved around POW hashing, we can assume that a similarly extreme efficiency will be achieved around storing files.

\section{Building Blocks - Overview}
We wish to use the following building blocks to create a federated storage system.
\begin{itemize}
	\item Consensus: each node needs to be in agreement about the state of the network.
	\item Random Number Generation: the network must have some expensive source of random numbers, where expensive is clearly defined.
	\item Proof of Retrivability: hosts must be able to prove that they have access to the data they have been assigned to store.
	\item Proof of Capacity: The network is likely to have storage available that's not in use. An algorithm for proving the existence of empty storage can be useful.
	\item Proof of File Transfer: if a file is transfered from one party to another, there must be a way to resolve a dispute (where the uploader says they sent the file, the downloader says they did not receive it).
	\item Scalable Network Topology: there must be a way to figure out where each file is stored, such that the storage requirements and computational requirments per node are minimal.
\end{itemize}

Proofs of retrivability (also commonly called proof of storage) are fundamental to any byzantine file storage system.

Random number generation is useful in a byzantine federated system, because we wish to randomize which hosts store files in a way that is not manipulatable by either the hosts or the file uploaders.
We use this randomness so that network wide statistics about reliability can be trusted when uploading a set of files to the network.
Randomness also helps prevent a malicious host from targeting specific files.

Proofs of file transfer are useful in a byzantine federated system because there can be situations where one host is required to transfer a file to another host (perhaps the first host is deciding to power off), and there may be a dispute about whether the file was actually transferred.
It is useful to have a tool to guarantee that a file was or wasn't transferred.

Scalable network topology is needed because the end goal is to have billions to trillions of files stored across thousands to millions of datacenters, all within a trust free environment.
It will not suffice for each node to have a complete view of the network topology, as that is an extreme amount of overhead.
There must be a method for nodes to navagate a topology without needing to see the entire topology in a byzantine environment.

Consensus is of course the foundation of any distributed system.
Everybody must be able to agree on the current state of the system.

\section{Consensus}
Two forms of consensus have been considered for Sia.
The first is POW based blockchain consensus, which has been tested by Bitcoin and proven to be sufficient in real world conditions.
The second is message passing consensus, in the form of byzantine paxos or similar algorithms.

The advantage to message passing consesus is the lack of need for mining rigs churning out mostly useless work.
The disadvantage is that message passing consensus often has more relaxed assumptions about what portion of the network is malicious, has more strict synchronization requirements, and has no protection against someone spoofing a very long alternate history for the network, which could be used to confuse/trick newcomers and outsiders.

Sia has chosen a hybrid model, using message passing consensus to manage network topology and to obserse host compliance (uptime, resource proofs), while using proof of work as the foundation of consensus, to prevent alternate histories from being trivially created.

This section will probably go into greater detail in future drafts.

\section{Random Number Generation}

\section{Proof of Storage}
Since we wish to distribute storage to foreign hosts, we must have a method to force the hosts to prove that they are storing the files that they are in charge of.
We will do this by taking a merkle hash of the file, hashing every 64 bytes into a 32 bytes hash, forming a tree.
The network topology will store a list of hosts and the merkle roots of every file that each host is responsible for storing.
A challenge can then be periodically issued to the hosts, asking them to prove that they have a random 64 byte segment of the file.
The proof is constructed by providing the 64 byte piece, and every hash in the tree that is required to build up to the merkle root.
The size of the proof is 64 bytes + log(filesize / 64 bytes) * 32 bytes.
On Sia, the maximum file size is limited to 25MB, which means the maximum proof size is 480 bytes (64 bytes, plus 13 tiers of merkle tree hashes, one hash per tier at 32 bytes each).

A proof does not have to be submitted for every file however, a file can be randomly selected from a host's set of files.
If a host can complete many consecutive storage proofs on random files that are being stored, it is very likely that the host is storing all or almost all of the files it is supposed to store.
This does give a host the opportunity to throw out a very small percentage of the data that it's supposed to be storing with a low probability of detection.

There is also a paper called 'Compact Proofs of Retrievability', which may provide a more compact way to do storage proofs.
Because merkle tree proofs are sufficient, this is one of the last things that we will visit for protocol improvement.

\section{Proof of Capacity}
It would be convenient for Sia to be able to measure the volume of free space on the network.
This would be best done using some sort of proof-of-capacity.
So far nothing efficient has been contrived.

\section{Proof of File Transfer}
There are many cases in which we wish to transfer files from one host to another.
Sometimes, there may be a dispute, where the uploader claims to have uploaded the file, and the downloader claims not to have received it.
Or, there may be claims of connectivity problems.

We have a solution that involves the use of moderators.
If there is a dispute, moderators are called in at random from the network.
The uploader uploads the file to the moderators, who then pass the file along to the downloader.
If the moderators get the file from the uploader, they sign that the transaction was attempted by the uploader, and that the downloader is at fault.
If half or more of the moderators sign, then the downloader is penalized.
Otherwise, the uploader is penalized.

Assuming that 51\% of the chosen moderators behave honestly, this provides a resolution to disputes that is secure.
If one side feels that the moderators were not 51\% honest, it is possible to bring in additional moderators, which makes it less likely that a minority of dishonest hosts can form a majority of moderators.
The request for more moderators will only be honored if statistical analysis reveals that it's possible that the batch of moderators was bad. (If 80\% of mods agree, and only 5 were chosen, then a bad batch of mods is possible. If 80\% of mods agree, and 15 have been chosen, a bad batch is less likely.)
Eventually, requests for more moderators will stop being honored, and the decision will be final.
An optimal number needs to be chosen.
Using moderators to assist uploads is very expensive.

One problem with the moderator approach is that there's currently nothing incentivizing the moderators to be honest about the completetion of the upload.
The uploader or downloader could potentially bribe the miner to lie in their favor, thus dodging the penalty and thrusting it upon the other party.
The incentive structure of this approach needs to be tuned.

\section{Federated Storage}
We wish to build a network of hosts and files, where every host stores a set of files, and every file is stored by exactly one host.
This network is dynamic, meaning that files can be added at any time, and hosts can be added at any time.
We further wish to perform consensus operations on this network.
To prevent Sybil attacks, hosts must submit storage proofs to be eligible to participate in consensus.
In order to protect consensus, we only wish to make sure that no malicious party can gain control of a majority of the voting power.

We make the following assumptions:
\begin{itemize}
	\item All non-malicious files are fully compressed and non-redundant.
	\item No party can afford more than 50\% of the total resources available to the network. That is, a party cannot perform a 50\% attack by buying enough storage hardware, renting 50\% of the storage available on the network, or some combination of the two.
	\item Renting a volume of storage on the network is at least as expensive as renting the same volume off the network.
\end{itemize}

Given these assumptions, we wish to build a network that satisfies the following properties:
\begin{itemize}
	\item Every file is stored on a host.
	\item On a sufficiently large network, the variance in the number of files stored by each host is bounded.
	\item A party controlling less than 50\% of the physical storage on the network cannot appear to control greater than 50\% of the storage without incurring greater economic cost than the cost of owning 50\% of the storage.
\end{itemize}

We establish that the following condition is sufficient to achieve the third property:
\begin{itemize}
	\item A set of hosts that comprise less than 50\% of the network cannot probabilistically control greater than 50\% of the files they have uploaded at any time without incurring prohibitive economic cost.
\end{itemize}
Because we assume a mechanism for proof of storage, we hold that the only way for a party to appear to control greater storage than it actually controls is to through the use of a specially constructed ``fake file'' whose storage proof can be spoofed.
This allows the malicious party to pretend to store the file without needing to consume physical storage.
To prevent such attacks, we must ensure that the cost of spoofing the storage proof is more expensive than the cost of genuinely storing the file, which is what we will define as prohibitive economic cost.

The fake file attack requires that the fake file uploaded to the network land on a conspiring host.
If the fake file has a greater chance of being on a non-conspiring host than a conspiring host, the cost of keeping the fake file will exceed the cost required to just buy raw storage.
If a set of hosts comprising less than 50\% of the network cannot store greater than 50\% of the fake files they upload without incurring prohibitive economic cost, then they cannot appear to control greater than 50\% of the raw storage space on the network.

The remainder of the paper shall design a system that prevents such an attack. The following properties will be achieved (given the prior assumptions):
\begin{enumerate}
	\item Every file is stored on a host.
	\item A set of hosts that comprise less than 50\% of the network cannot probabilistically control greater than 50\% of the files they have uploaded at any time without incurring prohibitive economic cost.
	\item On a sufficiently large network, the variance in the number of files stored by each host is bounded.
\end{enumerate}

Throughout this paper, only the quantity of files is discussed, with no mention of how large these files might be.
It can be assumed that all files are given a weight equal to their size in bytes when calculating penalties or making other considerations.
Additionally, hosts are discussed, with no mention of how much each host is stored.
It can be assumed that hosts are given a weight equal to the volume of files that they are storing.
Parts of this paper mention numbers like 10,000 files per host.
It can be assumed that there is a minimum volume of files that need to be stored, and that files have a maximum size.

\section{Rendezvouz Hashing Scheme}
A modified rendezvouz hashing scheme will be used to achieve the aforementioned goals.
Every host and file on the network is paired with a trusted random seed after joining the network.
The source of these seeds is outside the scope of this paper, though something like the hash of a future Bitcoin block could be used.
To determine which host stores a file, each host is assigned a number determined by concatenating its seed with the seed of file and hashing the result.
The host with the largest hash is chosen to store the file.

To achieve the first property, we simply require that at least one host be on the network at all times.

Achieving the second property requires enumerating all of the ways in which the network can reconfigure, and then demonstrating that none of the reconfigurations are vulnerable to manipulation.
The network will reconfigure during the following actions:
\begin{itemize}
	\item A file is added to the network.
	\item A file is removed from the network.
	\item A host is added to the network.
	\item A host is removed from the network.
\end{itemize}
For simplicity, we mandate that only one of these actions can happen at a time.

\subsection{Adding and Removing Files}
When a file is added to the network, it is given a random seed which will determine which host is responsible for storing it.
This process is sufficiently random, unless a conspirator tries to ``reroll'' the seed of a file by repeatedly adding and removing it from the network.
We limit this by forcing files to be pre-paid, preventing funds from being added to a file after it has been uploaded, and preventing a file from being removed from the network until the funds are depleted.
We also prevent the contents of the file from being altered, so that a conspiring uploader cannot edit a file to be ``fake'' after uploading it.
(This attack is only useful if the attacker intends to upload legitimate files anyway; files that land on conspiring hosts are made fake, and the original file is reuploaded to the network. Various social engineering attacks, like offering cheap storage, could be used to help the attacker locate a source of legitimate files that provide extra funds for the attack.)
With these limitations, there is no way for an uploader to manipulate the probability of a file landing on a conspiring host.

Files can be removed from the network prematurely if (and only if) the host that was storing the file loses the file.
When this occurs, the uploader is refunded for the remaining time that the file should have spent on the network.
A conspiring host can corrupt or remove valid files in order to increase the percentage of fake files it is storing.
When a file is lost or corrupt, the host is no longer responsible for storing it.
Therefore, a penalty must be incurred on the host that is as economically expensive as holding onto the file for the duration of the file's life.
This is as simple as forcing the host to refund the uploader an amount equal to the funds remaining in the file.
This holds even if the file had enough funds to outlive the host; at the end of a hosts life all of its files are transferred to another location on the network.

Using this penalty has a nice property: it allows hosts to selectively corrupt files without having to leave the network entirely.
This is good for legal reasons, as it allows hosts under legal pressure to delete files that are illegal without being forced away from participation in the network.
This may satisfy potential participants who are concerned about the legal and moral responsibilities accompanying hosting potentially objectionable files.
It is also noted that this property does not eliminate privacy: through encryption and obfuscation, an uploader can protect their own privacy, while still allowing law enforcement to prevent the distribution of illegal content without disrupting the foundational functioning of the network.

\subsection{Adding Hosts}
When a host joins the network, it receives a random set of files from throughout the network, taking them from random places.
The host can manipulate the outcome of this random event by leaving and rejoining, effectively rerolling the set of files it receives.
If we assume that the host is performing a reroll to increase the likelihood of receiving fake files, we can penalize the host for leaving early a greater amount than the host stands to gain by performing a reroll.
We can define ``leaving early'' by forcing the host to commit to an exact amount of time of being on the network.
Once that time has expired, the host is forced to leave.
If the host leaves before that time has expired, the host has left early.
The host stands to gain the most from doing a single reroll (multiple rerolls will have diminishing returns), so the network will assume that the host only intends to reroll once on average when determining the penalty.
We observe that a single reroll means that the host will get a better arrangement of files with approximately half of a standard deviation, meaning if the standard deviation for the number of fake files the host expects to get is 100, then a single reroll means the host can expect to get 50 fake files more than what would be expected without doing any rerolls.
The network can assume that no more than 50\% of the files are fake.
The network can calculate the standard deviation by taking the square root of 50\% of the files, times the probability of getting each file, times the inverse of that probability.
The penalty for leaving early is then defined by the cost of storing a half standard deviation of files for the amount of time that the host had remaining.

\begin{equation}
Penalty = \frac{1}{2} * \sqrt{(fakes) * \frac{1}{hosts} * (1 - \frac{1}{hosts})}
\end{equation}

On networks where hosts are expected to have a large number of files, the standard deviation is very small compared to the number of files that the host is storing.
At 10,000 files per host, the above equation works out to a penalty of 70.6 files.
70.6 is a small fraction of 10,000, and so a host storing an expected 10,000 files will have at most a penalty that is less than 1\% of the total expected income for storing the files.
This penalty is a reasonable risk for honest hosts to take.

\subsection{Removing Hosts}
When a host leaves the network, whether early or late, it has the responsibility of transferring all files to the next owner.
The host has the ability to decide not to transfer the file, which is the same as corrupting the file.
Preventing manipulation upon leaving the network is already covered if the same file corruption penalties are applied to hosts who do not properly transfer a file to their successor upon leaving the network.

We want to give hosts a way to deal with file corruption that does not result in them personally paying out for the full refund of the files lost, as this is prohibitively expensive.
The purpose of the penalty is not to discourage corruption, but instead to discourage manipulation of file distribution.
In the case of wholesale corruption, the goal is either to remove files from a host that has a high volume of valid files, or to prevent a high volume of valid files from being sent to other conspiring hosts on the network, or to prevent a high-volume of fake files from going to non-conspiring hosts on the network.
For this case, we need to look at the high probability cases, because a host can predict the outcome of wholesale corruption before executing, meaning that they can choose to execute the corruption only in favorable or unusual cases.
We do not care about extremely unusual cases, because a conspiring set of hosts will have very few opportunities to execute on these, and cannot use them to adjust the network by more than a fraction of a percent.
We will establish ``extremely unusual'' as being 4 standard deviations outside of the expected, which will happen 0.003\% of the time.
A host moving files favorably at 4 standard deviations, in the case of 10,000 files per host, will be manipulating around 560 files.
Therefore we set the penalty of wholesale corruption equivalent to the benefit of moving 560 files, or about 6\%, where you pay for the average time remaining on the files.
Though a bit scarier than the leaving early penalty, the wholesale corruption penalty is manageable.

\subsection{Distribution of Files Per Host}
The distribution of files cannot be disrupted by adding files to the network, as files join with a random seed, are sent to random hosts, and are stored for a fixed amount of time.
The distribution of files can be disrupted however by removing hosts when the hosts get an unfavorable random seed, or are able have a set of files sent to a favorable location by leaving,
All of the penalties associated with manipulating the distribution of files are expensive, regardless of whether the goal of manipulation involves fake files or not.
A conspiring set of hosts comprising 50\% of the hosts on the network looking to cause a disparity of 5\% between conspiring and non-conspiring hosts would have to move the network 2.5 standard deviations in each direction (assuming 10,000 files per host).
Shifting the average conspiring host by 2.5 standard deviations would require well over 100 rerolls per host, something that we can assume is economically prohibitive.
In conclusion, disrupting the distribution of files per host beyond any reasonable value (defined as a 5\% gap) is economically prohibitive.

\subsection{Summary of Rendezvouz Hashing}
A full summary of the rules of the network follows:
\begin{itemize}
	\item Files and hosts join the network with a precise, prepaid lifetime.
	\item After joining, each file and host is given a random seed.
	\item Files are placed onto hosts using the random seeds and rendezvouz hashing.
	\item Hosts are penalized for losing files ('corrupting files') an amount equivalent to how much time the file had remaining on the network, and the uploader is refunded.
	\item Hosts are penalized for leaving early equivalent to how useful a reroll might be in changing the distribution of fake files on the hosts machine.
	\item Hosts are penalized for 4 standard deviations of file redistribution if they lose all of the files --- and they are pardoned from any penalties for individual files in that set.
\end{itemize}
This set of rules enforces the three conditions established at the beginning of the paper, given the assumptions established at the beginning of the paper.

We observe four shortcomings with this approach:
\begin{itemize}
	\item Files need to obtain new seeds after their original payment expires. This causes turbulence, as the expired file will need to be uploaded to a new location.
	\item Hosts need to obtain new seeds after their allotted time expires. This causes turbulence, as the host will need to download a new set of files.
	\item Hosts can be penalized, and there must be a reliable way to enforce the penalties.
	\item The number of hashes needed to determine the state of the network is the number of hosts times the number of files. This will not scale.
\end{itemize}

\section{Topology Consensus}
A byzantine network needs some method for achieving consensus around the network topology.
Multiple algorithms exist for maintaining byzantine fault tolerant consensus, and are employed by existing cryptocurrencies.
By looking at how many files a host is storing, we can assign a volume of voting power to a host.
Hosts can then participate in consensus to have a combined perception of the network topology.

\section{Balanced Tree Distribution}
There are two valuable properties to the rendezvouz hashing that make it work in a Byzantine environment:
\begin{itemize}
	\item When adding a host to the network, the host has an equal chance of receiving every single file in the network.
	\item When adding a file to the network, the file has an equal chance of landing on every host in the network.
\end{itemize}
We can reduce the volume of hashing required by organizing hosts into a tree, and putting files on the hosts.
Because hosts can leave at any time, this tree is not guaranteed to be balanced.
However, it is known how many hosts remain on each side of the tree at all times.
When a file is added, it is given a random seed to determine which host it shall end up on.
At each level of the tree, the file will use to randomly pick a side weighted according to the total number of hosts in each side of the tree.
The seed is then hashed to produce a new seed for picking a direction at the next level.
When the file hits the bottom of the tree, it will be on a random host.

When a host is added to the network, it is placed on the lightest side of the tree.
This helps to maintain balance, and does not disrupt the randomness of the network because hosts do not draw files from their neighbors.
Instead, the host uses a random seed to pull files from around the network.
To get a file, the host hashes the seed and uses that as a random number to transverse the tree, similar to adding a file.
When it arrives at a host, it will draw a file at random from the host, using the next iteration of the original seed to pick a file.
This will be repeated for every file that the host draws, and the host will draw files until it has pulled a number of files equivalent the amount of storage it is bringing to the network.
The host will need to leave some extra space for new files that join the network and have a chance of landing of the host.
It is left to the host to optimize this process.

When a host leaves the network, its files are assigned new random seeds and given a new position on the network, which eliminates the possibility of leaving hosts to know where on the network the files will end up.

A few notes on this setup:
\begin{itemize}
	\item Files don't have a permanent seed anymore. They use a random seed to get placed, but then they tether to that host, getting a new seed if the host disappears.
	\item Where a host is in the tree doesn't matter. The tree can be arranged arbitrarily, and the host will just bring its files along.
	\item The same penalties for leaving early or losing files apply as in the rendezvouz hashing.
\end{itemize}

\section{Conclusion}
A brief description of the system, close to a protocol but less well defined, followed by a summary of the weaknesses of the protocol.

\end{document}

