\documentclass[twocolumn]{article}

\begin{document}
\frenchspacing

\title{Byzantine Federated Storage}

\author{
{\rm David Vorick}\\
Nebulous Labs
}

\maketitle

\subsection*{Abstract}
We explore a method of randomly distributing files to hosts in a dynamic system, in a way that prevents malicious hosts from manipulating the distribution of files on the network under certain constraints.

\section{Introduction}
We wish to build a network of hosts and files, where every host stores a set of files, and every file is stored by exactly one host.
This network is dynamic, meaning that files can be added at any time, and hosts can be added at any time.
We further wish to perform consensus operations on this network.
To prevent Sybil attacks, hosts must submit storage proofs to be eligible to participate in consensus.
In order to protect consensus, we only wish to make sure that no malicious party can gain control of a majority of the voting power.

We make the following assumptions:
\begin{itemize}
	\item No party can control greater than 50\% of the raw storage on the network.
	\item All non-malicious files are fully compressed and non-redundant.
	\item A mechanism for proof of storage exists.
	\item No party can afford more than 50\% of the total resources available to the network. That is, a party cannot perform a 50\% attack by buying enough storage hardware, renting 50\% of the storage available on the network, or some combination of the two.
	\item Renting a volume of storage on the network is at least as expensive as renting the same volume off the network.
\end{itemize}

Given these assumptions, we wish to build a network that satisfies the following properties:
\begin{itemize}
	\item Every file is stored on a host.
    \item The distribution of files amongst hosts is normal (Gaussian).
	\item A party controlling less than 50\% of the storage on the network cannot appear to control greater than 50\% of the storage without spending more money than the cost of owning 50\% of the storage.
\end{itemize}

We establish that the following two conditions are sufficient to prove the third property:
\begin{itemize}
	\item A set of hosts that comprise less than 50\% of the network cannot probabilistically control greater than 50\% of the files they have uploaded at any time.
	\item A set of hosts controlling less than 50\% of the files on the network cannot perform any action which results in the expected percentage of files across all machines under their control to exceed 50\% of the files under their control.
\end{itemize}
Because we assume a mechanism for proof of storage, we hold that the only way for a party to appear to control greater storage than it actually controls is to through the use of a specially constructed ``fake file'' whose storage proof can be spoofed.
This allows the malicious party to pretend to store the file without needing to consume physical storage.
To prevent such attacks, we must ensure that the cost of spoofing the storage proof is more expensive than the cost of genuinely storing the file.

Note that this attack requires that the attacker be responsible for storing the fake file that it uploads.
Thus, we must prove that under the replacement conditions, a host containing only enough economic power to control 50\% of the files XOR storage on the network will not be able to inflate their apparent storage above 50\%.
If a host controlling less than 50\% of the network cannot probabilistically have 50\% of files landing on machines in their control, they can only fake storage on less than 50\% of the files that they are uploading.
Because the cost of storage on the network is equivalent or greater than the cost of raw storage, and the attacker cannot probabilistically store more than 50\% of their own fake files, they cannot save money by using this attack.
So while the malicious party is able to appear that they have more raw storage than they actually have, this would require them to spend more than the total cost of resources available to the network, which contradicts assumption 4.
Therefore, if an attacker cannot probabilistically store greater than 50\% of the files they upload when they control less than 50\% of the machines, they cannot appear to store greater than 50\% of the network via this attack.

The only other approach to faking storage is to own a large number of files, and to add machines to the network in a way that allows the machines to consist of more than 50\% fake files.
At that percentage, apparent storage can be added to the network at a cost less than raw storage.
By preventing this from happening, the party has no other way to increase their apparent storage beyond 50\% of the network that is within their assumed economic power.
The remainder of the paper shall design a system that prevents such an attack. The following properties will be achieved (given the prior assumptions):
\begin{itemize}
	\item Every file is stored on a host.
	\item A set of hosts less than 50\% of the network cannot probabilistically have a greater than 50\% of the files they have uploaded under control of their machines at any time.
	\item A set of hosts controlling less than 50\% of the files on the network cannot perform any action which results in the expected percentage of files across all machines under their control to exceed 50\% of the files under their control.
	\item On a sufficiently large network, the variance in the number of files stored by each host is bounded.
\end{itemize}

\section{A Non-Scaling Solution}
A modified rendezvouz hashing scheme will be used to achieve the aforementioned goals.
Every host and file on the network is paired with a trusted random seed.
The source of these seeds is outside the scope of this paper, though something like the hash of the next future Bitcoin block could be used.
To determine which host stores a file, each host is assigned a number determined by concatenating its seed with the seed of file and hashing the result.
The host with the largest hash is chosen to store the file.
Because this process assigns files to hosts in a suitably random fashion, property 4 is achieved.

To achieve the property that every file is stored on a host, we simply require that at least one host be on the network at all times.

The remaining properties are slightly trickier, as hosts and files can attempt to manipulate the trusted random number by forcing rerolls.
After joining the network, the random number is checked, and if it is unfavorable, the attacker simply leaves the network and tries again.
The solution for files is easy: the cost of storage must be paid before the random number is rolled, and once placed on the network, a file cannot add more to the storage balance, nor can the storage balance be recovered.
This takes care of the second condition.
If a set of hosts less than 50\% of the total hosts uploads a file, it will probabilistically end up on a machine that is not controlled by the hosts.
The hosts will have to pay more for uploading fake files than they will economically recover, which keeps the network protected.

Unfortunately, the third property cannot be satisfied using this approach, because hosts can withdraw and rejoin at will; nothing is forcing a host to stick around and complete storage proofs.
In situations where the number of random files on the network is large, a joining host can re-join multiple times until it receives a favorable set.
This can be prevented by forcing the host to make a down payment, which is returned linearly as the host completes the storage proofs.
The down payment must be large enough that the cost of leaving is greater than the probabilistic reward of performing a reroll.
Therefore, if 50\% of the files are controlled by the party forcing the reroll, the minimum down payment must be equal to the expected return on one reroll.
The expected return on 1 reroll is dependent on the number of files on the network, and the number of files being sent to the host.
When the number of files is large, and the number of files stored on the host is large, the expected return on 1 reroll will be small, so security in this manner will be cheap for large networks.
After the host has completed the storage proofs, they must make another down payment and roll a new random seed.

The distribution of files cannot be disrupted by adding files to the network, as files join with a random seed, are sent to random hosts, and are stored for a fixed amount of time.
However, the distribution of files can be disrupted by adding \textit{hosts} to the network, and then removing them if the number of files assigned to those hosts is unfavorable.
Significant distribution manipulation takes significant economic power, and takes a larger volume of rerolls when the number of files per machine is large, because the variance as a percentage will be much smaller.
A single host performing around 7 rerolls will be able to be one standard deviation of advantage.
Assuming 100 files per host, the standard deviation will be 10 files per host.
Assuming that the manipulator controls nearly 50\% of the hosts on the network, and can afford 7 rerolls per host, the attacker will be able to affect the distribution of the files on the network by 1 standard deviation per direction, such that non-party machines have ~110 files each (or 90 files each) and party machines have ~90 files each (or 110 files each, depending on the goals of the attack).
Adding more files per host results in a lower percent deviation (1000 files per host has a standard deviation of 31.6 files), resulting in reduced ability of an attacker to manipulate the distribution of files.
And this is overly generous; once an attacker has manipulated the distribution of files in a direction, a newcomer is more likely to pull files from the heavy side of the network than the light side of the network; it will take even more rerolls of the random seed to get a set of files that further stretches the distribution of files on the network.

If this attack vector is a significant concern, the required down payment for hosts on the network can be increased, or the number of files per host can be increased.

One final rule is noted on the storage proofs.
A host either has the whole set of files, or it doesn't, and a host cannot report that a single file has corrupted.
If the host claims a single file has corrupted, the host is considered to have failed the storage proof and must pay the same penalty as if it was being non-cooperative.
This is to prevent hosts from selectively manipulating files to boost the percentage of compromised files under their control.

We observe 4 shortcomings with this approach:
\begin{itemize}
	\item Files need to get new seeds after their original payment expires. This causes turbulence.
	\item Hosts need to get new seeds after their original payment expires. This causes turbulence.
	\item Hosts need to make a down payment. This discourages hosts from joining.
	\item The number of hashes needed to determine the state of the network is the number of hosts times the number of files. This will not scale.
\end{itemize}
A better scheme should be looked for.

\iffalse
\section{Addressing Scheme}
There is a 80 bit addressing space which helps to determine which hosts will store which files, containing $2^{80}$ slots for files.
Collisions are permissable - multiple files or hosts can be in the same slot.
This address space is only partially in use at a time, depending on the number of hosts in the network. For each host, $2^{24}$ slots are open on the network.
Each host has a 'spanning area' of $2^{30}$ slots, cenetered around the hosts 'home' slot. This means that there is an overlap of $2^6$ - each slot on the network will probabilistically be covered by $2^6$ hosts.
A hosts home slot will never change.
This is a feature of the network because if a host's home slot were to change, the files which it stores would also change entirely, resulting in a large bandwidth expense to the network, which is undesirable.

When a file is added to the network, it is assigned a random seed 256 bits in length.
The seed is assigend from an external source of entropy - it is not the hash of the file nor has any relationship to the file.
This seed is truncated to produce a random slot in the 80 bit address space.
If the slot assigned to the file is outside of the active space, the hash of the seed is taken and used to produce a new slot between the 0th slot and the initial slot.
This process is repeated until the file ends in a slot within the active address space.
\textbf{Example:} We have a full address space of 512 slots with an active space of 100 slots.
A new file is randomly assigned a seed that is used to produce a random slot between the 0th slot
The seed is hashed to prodouce a new value, and this value is used to choose a new slot between index 0 and index 386 (inclusive).
The new slot is 105, still outside of the address space.
This process is repeated until a slot value appears that is between 0 and 100 inclusive.
The expected number of hashes is approximately equivalent to the log of the number of slots that are inactive minus the log of the slots that are active.
Because the process itself uses the hashes of a trusted random seed, and stops preciely when a value is first available in active space, which space of the active spaces gets chosen is completely random.
Therefore, adding a file to the network will always result in the file being placed in a random slot.

When a host is added to the network, the active addresses are expanded by $2^{24}$ slots.
The host is then given a random seed derived from an external source of entropy.
This seed is used to determine a permanent slot for the host to reside.

As a host is added to the network, new address space is opened up.
We wish to put files in this new address space at random, taken from the other slots on the network.
This is achieved by looking at the iterated hashes used to find a file's current location.
If a slot in a previous iteration opens up, that slot is given priority for the file, and so the file is moved from its current address to the now-available space.

When a host leaves the network, every file that the host was storing is given a completely new seed.
\fi

\end{document}

